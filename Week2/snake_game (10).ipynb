{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEUOSuOtPAu"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import pygame\n",
        "import random\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, state_space, action_space):\n",
        "        super().__init__()\n",
        "        # Add your architecture parameters here\n",
        "        # You can use nn.Functional\n",
        "        # Remember that the input is of size batch_size x state_space\n",
        "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
        "        # TODO: Add code here\n",
        "        self.fc1 = nn.Linear(state_space, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, action_space)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Complete based on your implementation\n",
        "        x = input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n"
      ],
      "metadata": {
        "id": "jOS7NefhtV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# While training neural networks, we split the data into batches.\n",
        "# To improve the training, we need to remove the \"correlation\" between game states\n",
        "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
        "# states at random which reduces the correlation.\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "cCuiyTa5tXle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SnakeGame(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, size=10, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.cell_size = 30\n",
        "        self.screen_size = self.size * self.cell_size\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
        "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size,3), dtype=np.uint8)\n",
        "\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.score = 0\n",
        "\n",
        "        self.snake = deque()\n",
        "        self.food = None\n",
        "        self.direction = 0\n",
        "\n",
        "        self.done = False\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.snake.clear()\n",
        "        mid = self.size // 2\n",
        "        self.snake.appendleft((mid, mid))\n",
        "        self.direction = 0\n",
        "        self._place_food()\n",
        "        self.done = False\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_init()\n",
        "\n",
        "        # print(f\"Reset: Snake at {tuple(self.snake)}, Food at {self.food}, Direction: {self.direction}\")\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Change reward schema to avoid the following\n",
        "        # 1) 180 degree turns\n",
        "        # 2) Wall collisions\n",
        "        # 3) Being slow at collecting food\n",
        "\n",
        "        # print(f\"--- Step ---\\nAction: {action}, Direction: {self.direction}\")\n",
        "        # print(f\"Snake before: {list(self.snake)}\")\n",
        "\n",
        "        if self.done:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        # Prevent 180-degree turns by reverting to previous direction if attempted\n",
        "        if (action + 2) % 4 == self.direction:\n",
        "          action = self.direction\n",
        "        self.direction = action\n",
        "\n",
        "        # [right, up, left, down]:\n",
        "        move = [(1, 0), (0, -1), (-1, 0), (0, 1)][self.direction]\n",
        "        head_x, head_y = self.snake[0]\n",
        "        new_head = (head_x + move[0], head_y + move[1])\n",
        "\n",
        "\n",
        "        if (not 0 <= new_head[0] < self.size) or (not 0 <= new_head[1] < self.size) or (new_head in self.snake):\n",
        "            self.done = True\n",
        "            return self._get_obs(), -10, True, {}\n",
        "\n",
        "        self.snake.insert(0, new_head)\n",
        "\n",
        "        if new_head == self.food:\n",
        "            self.score += 1\n",
        "            reward = 10\n",
        "            self._place_food()\n",
        "            print(f\"Food eaten at {self.food}! Current score: {self.score}, reward: {reward}\")\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "            reward = -0.1  # small penalty to encourage faster food collection\n",
        "\n",
        "        return self._get_obs(), reward, False, {}\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Return an observation state, take inspiration from the observation_space attribute\n",
        "        grid = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n",
        "        # print(\"hi\")\n",
        "        for x, y in self.snake:\n",
        "            grid[y, x, 0] = 1  # snake body in red channel\n",
        "        fx, fy = self.food\n",
        "        grid[fy, fx, 1] = 1  # food in green channel\n",
        "        return grid\n",
        "\n",
        "    def _place_food(self):\n",
        "        positions = set(tuple(p) for p in self.snake)\n",
        "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
        "        self.food = tuple(random.choice(empty)) if empty else None\n",
        "\n",
        "    def render(self):\n",
        "        if self.screen is None:\n",
        "            self._render_init()\n",
        "\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        for x, y in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (0, 255, 0),\n",
        "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (255, 0, 0),\n",
        "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def _render_init(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen:\n",
        "            pygame.quit()\n",
        "            self.screen = None"
      ],
      "metadata": {
        "id": "fgUqSwDptZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_snake(episodes=300):\n",
        "    env = SnakeGame(size=10)\n",
        "    obs_shape = env.observation_space.shape  # (10, 10, 3)\n",
        "    state_dim = np.prod(obs_shape)           # Flattened input: 300\n",
        "    action_dim = env.action_space.n          # 4 actions\n",
        "\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    target_model = DQN(state_dim, action_dim)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ExperienceBuffer(5000)\n",
        "\n",
        "    batch_size = 32\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    min_epsilon = 0.05\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        state = obs.flatten().astype(np.float32)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, action_dim - 1)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_vals = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "                    action = torch.argmax(q_vals).item()\n",
        "\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            next_state = next_obs.flatten().astype(np.float32)\n",
        "\n",
        "            buffer.push(state.copy(), action, reward, next_state.copy(), done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                next_q = target_model(next_states).max(1, keepdim=True)[0].detach()\n",
        "                target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "            print(f\"Episode {ep}: Total Reward = {total_reward}\")\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        # print(epsilon)\n",
        "\n",
        "    env.close()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "teCCCqyHtcPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_snake_model(model, size=10, episodes=10, render=True):\n",
        "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs.flatten(), dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ],
      "metadata": {
        "id": "401_rsnjtdp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run evaluation for Snake Game here\n",
        "\n",
        "torch.cuda.empty_cache()  # if using GPU\n",
        "model = train_snake()\n",
        "evaluate_snake_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-I3aG2StfIt",
        "outputId": "056dbce5-4403-441d-b4ed-0c86bd1ff4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0: Total Reward = -10.7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4021931876.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Food eaten at (0, 7)! Current score: 1, reward: 10\n",
            "Food eaten at (1, 0)! Current score: 2, reward: 10\n",
            "Episode 10: Total Reward = -10.6\n",
            "Food eaten at (0, 9)! Current score: 3, reward: 10\n",
            "Food eaten at (1, 3)! Current score: 4, reward: 10\n",
            "Episode 20: Total Reward = -11.9\n",
            "Episode 30: Total Reward = -11.1\n",
            "Food eaten at (5, 4)! Current score: 5, reward: 10\n",
            "Food eaten at (4, 7)! Current score: 6, reward: 10\n",
            "Episode 40: Total Reward = -11.1\n",
            "Food eaten at (0, 5)! Current score: 7, reward: 10\n",
            "Episode 50: Total Reward = -10.7\n",
            "Food eaten at (8, 7)! Current score: 8, reward: 10\n",
            "Episode 60: Total Reward = -10.7\n",
            "Food eaten at (7, 4)! Current score: 9, reward: 10\n",
            "Episode 70: Total Reward = -10.7\n",
            "Episode 80: Total Reward = -10.5\n",
            "Episode 90: Total Reward = -15.299999999999997\n",
            "Food eaten at (3, 0)! Current score: 10, reward: 10\n",
            "Food eaten at (2, 8)! Current score: 11, reward: 10\n",
            "Episode 100: Total Reward = -10.5\n",
            "Food eaten at (3, 0)! Current score: 12, reward: 10\n",
            "Episode 110: Total Reward = -11.1\n",
            "Food eaten at (0, 2)! Current score: 13, reward: 10\n",
            "Episode 120: Total Reward = -10.5\n",
            "Episode 130: Total Reward = -10.9\n",
            "Food eaten at (6, 7)! Current score: 14, reward: 10\n",
            "Food eaten at (4, 3)! Current score: 15, reward: 10\n",
            "Food eaten at (2, 3)! Current score: 16, reward: 10\n",
            "Food eaten at (9, 6)! Current score: 17, reward: 10\n",
            "Episode 140: Total Reward = -2.5999999999999925\n",
            "Food eaten at (0, 9)! Current score: 18, reward: 10\n",
            "Food eaten at (6, 2)! Current score: 19, reward: 10\n",
            "Episode 150: Total Reward = -13.200000000000001\n",
            "Episode 160: Total Reward = -11.0\n",
            "Food eaten at (0, 9)! Current score: 20, reward: 10\n",
            "Food eaten at (1, 6)! Current score: 21, reward: 10\n",
            "Food eaten at (3, 4)! Current score: 22, reward: 10\n",
            "Episode 170: Total Reward = 8.199999999999996\n",
            "Episode 180: Total Reward = -11.2\n",
            "Food eaten at (3, 2)! Current score: 23, reward: 10\n",
            "Food eaten at (7, 8)! Current score: 24, reward: 10\n",
            "Food eaten at (3, 0)! Current score: 25, reward: 10\n",
            "Episode 190: Total Reward = -13.000000000000002\n",
            "Episode 200: Total Reward = -13.100000000000001\n",
            "Food eaten at (0, 7)! Current score: 26, reward: 10\n",
            "Food eaten at (9, 0)! Current score: 27, reward: 10\n",
            "Food eaten at (9, 8)! Current score: 28, reward: 10\n",
            "Food eaten at (3, 9)! Current score: 29, reward: 10\n",
            "Food eaten at (0, 9)! Current score: 30, reward: 10\n",
            "Episode 210: Total Reward = -0.8999999999999968\n",
            "Food eaten at (0, 8)! Current score: 31, reward: 10\n",
            "Food eaten at (4, 8)! Current score: 32, reward: 10\n",
            "Food eaten at (9, 4)! Current score: 33, reward: 10\n",
            "Food eaten at (9, 4)! Current score: 34, reward: 10\n",
            "Food eaten at (1, 5)! Current score: 35, reward: 10\n",
            "Food eaten at (8, 1)! Current score: 36, reward: 10\n",
            "Episode 220: Total Reward = -10.5\n",
            "Food eaten at (2, 2)! Current score: 37, reward: 10\n",
            "Food eaten at (5, 1)! Current score: 38, reward: 10\n",
            "Episode 230: Total Reward = -10.7\n",
            "Food eaten at (9, 1)! Current score: 39, reward: 10\n",
            "Episode 240: Total Reward = -10.9\n",
            "Food eaten at (9, 0)! Current score: 40, reward: 10\n",
            "Food eaten at (6, 7)! Current score: 41, reward: 10\n",
            "Food eaten at (9, 3)! Current score: 42, reward: 10\n",
            "Food eaten at (4, 6)! Current score: 43, reward: 10\n",
            "Episode 250: Total Reward = -10.5\n",
            "Food eaten at (9, 8)! Current score: 44, reward: 10\n",
            "Food eaten at (3, 6)! Current score: 45, reward: 10\n",
            "Food eaten at (4, 5)! Current score: 46, reward: 10\n",
            "Food eaten at (5, 8)! Current score: 47, reward: 10\n",
            "Episode 260: Total Reward = -11.5\n",
            "Food eaten at (3, 6)! Current score: 48, reward: 10\n",
            "Food eaten at (3, 9)! Current score: 49, reward: 10\n",
            "Food eaten at (7, 6)! Current score: 50, reward: 10\n",
            "Food eaten at (6, 4)! Current score: 51, reward: 10\n",
            "Food eaten at (3, 1)! Current score: 52, reward: 10\n",
            "Episode 270: Total Reward = 8.999999999999986\n",
            "Food eaten at (3, 3)! Current score: 53, reward: 10\n",
            "Food eaten at (6, 0)! Current score: 54, reward: 10\n",
            "Food eaten at (7, 7)! Current score: 55, reward: 10\n",
            "Episode 280: Total Reward = -14.3\n",
            "Food eaten at (2, 0)! Current score: 56, reward: 10\n",
            "Food eaten at (3, 1)! Current score: 57, reward: 10\n",
            "Food eaten at (3, 0)! Current score: 58, reward: 10\n",
            "Food eaten at (2, 8)! Current score: 59, reward: 10\n",
            "Food eaten at (0, 2)! Current score: 60, reward: 10\n",
            "Food eaten at (6, 1)! Current score: 61, reward: 10\n",
            "Episode 290: Total Reward = -22.79999999999997\n",
            "Food eaten at (9, 8)! Current score: 62, reward: 10\n",
            "Food eaten at (4, 7)! Current score: 63, reward: 10\n",
            "Food eaten at (5, 8)! Current score: 64, reward: 10\n",
            "Food eaten at (4, 0)! Current score: 65, reward: 10\n",
            "Food eaten at (1, 1)! Current score: 66, reward: 10\n"
          ]
        }
      ]
    }
  ]
}