{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca209422",
      "metadata": {
        "id": "ca209422"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import pygame\n",
        "import random\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddc73b4",
      "metadata": {
        "id": "eddc73b4"
      },
      "outputs": [],
      "source": [
        "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, state_space, action_space):\n",
        "        super().__init__()\n",
        "        # Add your architecture parameters here\n",
        "        # You can use nn.Functional\n",
        "        # Remember that the input is of size batch_size x state_space\n",
        "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
        "        # TODO: Add code here\n",
        "        self.fc1 = nn.Linear(state_space, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, action_space)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Complete based on your implementation\n",
        "        x = input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37adca73",
      "metadata": {
        "id": "37adca73"
      },
      "outputs": [],
      "source": [
        "# While training neural networks, we split the data into batches.\n",
        "# To improve the training, we need to remove the \"correlation\" between game states\n",
        "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
        "# states at random which reduces the correlation.\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe4d26a",
      "metadata": {
        "id": "7fe4d26a"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training logic for CartPole environment here\n",
        "# Remember to use the ExperienceBuffer and a target network\n",
        "# Details can be found in the book sent in the group\n",
        "\n",
        "def train_cartpole():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    target_model = DQN(state_dim, action_dim)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ExperienceBuffer(10000)\n",
        "\n",
        "    batch_size = 64\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    min_epsilon = 0.01\n",
        "\n",
        "    episodes = 500\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, action_dim - 1)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_vals = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "                    action = torch.argmax(q_vals).item()\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                next_q = target_model(next_states).max(1, keepdim=True)[0].detach()\n",
        "                target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        print(f\"Episode {ep + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6493553c",
      "metadata": {
        "id": "6493553c"
      },
      "outputs": [],
      "source": [
        "def evaluate_cartpole_model(model, episodes=10, render=True):\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8643afe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8643afe7",
        "outputId": "cbb3a66a-5285-467e-e35c-ab6a7ad83d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward = 27.0\n",
            "Episode 2: Reward = 33.0\n",
            "Episode 3: Reward = 13.0\n",
            "Episode 4: Reward = 22.0\n",
            "Episode 5: Reward = 11.0\n",
            "Episode 6: Reward = 29.0\n",
            "Episode 7: Reward = 15.0\n",
            "Episode 8: Reward = 39.0\n",
            "Episode 9: Reward = 14.0\n",
            "Episode 10: Reward = 16.0\n",
            "Episode 11: Reward = 20.0\n",
            "Episode 12: Reward = 86.0\n",
            "Episode 13: Reward = 49.0\n",
            "Episode 14: Reward = 14.0\n",
            "Episode 15: Reward = 24.0\n",
            "Episode 16: Reward = 30.0\n",
            "Episode 17: Reward = 25.0\n",
            "Episode 18: Reward = 17.0\n",
            "Episode 19: Reward = 28.0\n",
            "Episode 20: Reward = 17.0\n",
            "Episode 21: Reward = 21.0\n",
            "Episode 22: Reward = 13.0\n",
            "Episode 23: Reward = 21.0\n",
            "Episode 24: Reward = 38.0\n",
            "Episode 25: Reward = 32.0\n",
            "Episode 26: Reward = 14.0\n",
            "Episode 27: Reward = 27.0\n",
            "Episode 28: Reward = 33.0\n",
            "Episode 29: Reward = 10.0\n",
            "Episode 30: Reward = 22.0\n",
            "Episode 31: Reward = 54.0\n",
            "Episode 32: Reward = 15.0\n",
            "Episode 33: Reward = 16.0\n",
            "Episode 34: Reward = 15.0\n",
            "Episode 35: Reward = 30.0\n",
            "Episode 36: Reward = 39.0\n",
            "Episode 37: Reward = 23.0\n",
            "Episode 38: Reward = 21.0\n",
            "Episode 39: Reward = 10.0\n",
            "Episode 40: Reward = 31.0\n",
            "Episode 41: Reward = 14.0\n",
            "Episode 42: Reward = 19.0\n",
            "Episode 43: Reward = 20.0\n",
            "Episode 44: Reward = 56.0\n",
            "Episode 45: Reward = 45.0\n",
            "Episode 46: Reward = 39.0\n",
            "Episode 47: Reward = 26.0\n",
            "Episode 48: Reward = 29.0\n",
            "Episode 49: Reward = 29.0\n",
            "Episode 50: Reward = 27.0\n",
            "Episode 51: Reward = 22.0\n",
            "Episode 52: Reward = 40.0\n",
            "Episode 53: Reward = 26.0\n",
            "Episode 54: Reward = 98.0\n",
            "Episode 55: Reward = 76.0\n",
            "Episode 56: Reward = 21.0\n",
            "Episode 57: Reward = 49.0\n",
            "Episode 58: Reward = 125.0\n",
            "Episode 59: Reward = 101.0\n",
            "Episode 60: Reward = 12.0\n",
            "Episode 61: Reward = 23.0\n",
            "Episode 62: Reward = 73.0\n",
            "Episode 63: Reward = 20.0\n",
            "Episode 64: Reward = 24.0\n",
            "Episode 65: Reward = 82.0\n",
            "Episode 66: Reward = 25.0\n",
            "Episode 67: Reward = 14.0\n",
            "Episode 68: Reward = 14.0\n",
            "Episode 69: Reward = 14.0\n",
            "Episode 70: Reward = 45.0\n",
            "Episode 71: Reward = 50.0\n",
            "Episode 72: Reward = 17.0\n",
            "Episode 73: Reward = 25.0\n",
            "Episode 74: Reward = 90.0\n",
            "Episode 75: Reward = 46.0\n",
            "Episode 76: Reward = 59.0\n",
            "Episode 77: Reward = 29.0\n",
            "Episode 78: Reward = 39.0\n",
            "Episode 79: Reward = 79.0\n",
            "Episode 80: Reward = 47.0\n",
            "Episode 81: Reward = 68.0\n",
            "Episode 82: Reward = 41.0\n",
            "Episode 83: Reward = 18.0\n",
            "Episode 84: Reward = 82.0\n",
            "Episode 85: Reward = 33.0\n",
            "Episode 86: Reward = 115.0\n",
            "Episode 87: Reward = 23.0\n",
            "Episode 88: Reward = 41.0\n",
            "Episode 89: Reward = 56.0\n",
            "Episode 90: Reward = 60.0\n",
            "Episode 91: Reward = 50.0\n",
            "Episode 92: Reward = 74.0\n",
            "Episode 93: Reward = 97.0\n",
            "Episode 94: Reward = 22.0\n",
            "Episode 95: Reward = 62.0\n",
            "Episode 96: Reward = 35.0\n",
            "Episode 97: Reward = 57.0\n",
            "Episode 98: Reward = 13.0\n",
            "Episode 99: Reward = 43.0\n",
            "Episode 100: Reward = 37.0\n",
            "Episode 101: Reward = 61.0\n",
            "Episode 102: Reward = 121.0\n",
            "Episode 103: Reward = 132.0\n",
            "Episode 104: Reward = 94.0\n",
            "Episode 105: Reward = 50.0\n",
            "Episode 106: Reward = 37.0\n",
            "Episode 107: Reward = 25.0\n",
            "Episode 108: Reward = 21.0\n",
            "Episode 109: Reward = 69.0\n",
            "Episode 110: Reward = 110.0\n",
            "Episode 111: Reward = 18.0\n",
            "Episode 112: Reward = 44.0\n",
            "Episode 113: Reward = 124.0\n",
            "Episode 114: Reward = 10.0\n",
            "Episode 115: Reward = 85.0\n",
            "Episode 116: Reward = 33.0\n",
            "Episode 117: Reward = 19.0\n",
            "Episode 118: Reward = 165.0\n",
            "Episode 119: Reward = 94.0\n",
            "Episode 120: Reward = 52.0\n",
            "Episode 121: Reward = 11.0\n",
            "Episode 122: Reward = 59.0\n",
            "Episode 123: Reward = 143.0\n",
            "Episode 124: Reward = 25.0\n",
            "Episode 125: Reward = 60.0\n",
            "Episode 126: Reward = 46.0\n",
            "Episode 127: Reward = 12.0\n",
            "Episode 128: Reward = 73.0\n",
            "Episode 129: Reward = 88.0\n",
            "Episode 130: Reward = 56.0\n",
            "Episode 131: Reward = 323.0\n",
            "Episode 132: Reward = 115.0\n",
            "Episode 133: Reward = 9.0\n",
            "Episode 134: Reward = 69.0\n",
            "Episode 135: Reward = 102.0\n",
            "Episode 136: Reward = 84.0\n",
            "Episode 137: Reward = 124.0\n",
            "Episode 138: Reward = 71.0\n",
            "Episode 139: Reward = 119.0\n",
            "Episode 140: Reward = 69.0\n",
            "Episode 141: Reward = 178.0\n",
            "Episode 142: Reward = 185.0\n",
            "Episode 143: Reward = 345.0\n",
            "Episode 144: Reward = 266.0\n",
            "Episode 145: Reward = 74.0\n",
            "Episode 146: Reward = 67.0\n",
            "Episode 147: Reward = 53.0\n",
            "Episode 148: Reward = 260.0\n",
            "Episode 149: Reward = 78.0\n",
            "Episode 150: Reward = 364.0\n",
            "Episode 151: Reward = 197.0\n",
            "Episode 152: Reward = 52.0\n",
            "Episode 153: Reward = 102.0\n",
            "Episode 154: Reward = 15.0\n",
            "Episode 155: Reward = 204.0\n",
            "Episode 156: Reward = 89.0\n",
            "Episode 157: Reward = 66.0\n",
            "Episode 158: Reward = 64.0\n",
            "Episode 159: Reward = 83.0\n",
            "Episode 160: Reward = 111.0\n",
            "Episode 161: Reward = 180.0\n",
            "Episode 162: Reward = 112.0\n",
            "Episode 163: Reward = 24.0\n",
            "Episode 164: Reward = 22.0\n",
            "Episode 165: Reward = 151.0\n",
            "Episode 166: Reward = 145.0\n",
            "Episode 167: Reward = 147.0\n",
            "Episode 168: Reward = 12.0\n",
            "Episode 169: Reward = 189.0\n",
            "Episode 170: Reward = 271.0\n",
            "Episode 171: Reward = 14.0\n",
            "Episode 172: Reward = 17.0\n",
            "Episode 173: Reward = 108.0\n",
            "Episode 174: Reward = 16.0\n",
            "Episode 175: Reward = 96.0\n",
            "Episode 176: Reward = 101.0\n",
            "Episode 177: Reward = 15.0\n",
            "Episode 178: Reward = 129.0\n",
            "Episode 179: Reward = 110.0\n",
            "Episode 180: Reward = 22.0\n",
            "Episode 181: Reward = 90.0\n",
            "Episode 182: Reward = 121.0\n",
            "Episode 183: Reward = 92.0\n",
            "Episode 184: Reward = 105.0\n",
            "Episode 185: Reward = 21.0\n",
            "Episode 186: Reward = 28.0\n",
            "Episode 187: Reward = 60.0\n",
            "Episode 188: Reward = 72.0\n",
            "Episode 189: Reward = 17.0\n",
            "Episode 190: Reward = 111.0\n",
            "Episode 191: Reward = 35.0\n",
            "Episode 192: Reward = 64.0\n",
            "Episode 193: Reward = 89.0\n",
            "Episode 194: Reward = 87.0\n",
            "Episode 195: Reward = 31.0\n",
            "Episode 196: Reward = 58.0\n",
            "Episode 197: Reward = 109.0\n",
            "Episode 198: Reward = 109.0\n",
            "Episode 199: Reward = 105.0\n",
            "Episode 200: Reward = 113.0\n",
            "Episode 201: Reward = 104.0\n",
            "Episode 202: Reward = 19.0\n",
            "Episode 203: Reward = 70.0\n",
            "Episode 204: Reward = 74.0\n",
            "Episode 205: Reward = 100.0\n",
            "Episode 206: Reward = 115.0\n",
            "Episode 207: Reward = 104.0\n",
            "Episode 208: Reward = 13.0\n",
            "Episode 209: Reward = 15.0\n",
            "Episode 210: Reward = 120.0\n",
            "Episode 211: Reward = 124.0\n",
            "Episode 212: Reward = 23.0\n",
            "Episode 213: Reward = 122.0\n",
            "Episode 214: Reward = 78.0\n",
            "Episode 215: Reward = 172.0\n",
            "Episode 216: Reward = 124.0\n",
            "Episode 217: Reward = 46.0\n",
            "Episode 218: Reward = 127.0\n",
            "Episode 219: Reward = 13.0\n",
            "Episode 220: Reward = 22.0\n",
            "Episode 221: Reward = 110.0\n",
            "Episode 222: Reward = 15.0\n",
            "Episode 223: Reward = 79.0\n",
            "Episode 224: Reward = 55.0\n",
            "Episode 225: Reward = 22.0\n",
            "Episode 226: Reward = 68.0\n",
            "Episode 227: Reward = 111.0\n",
            "Episode 228: Reward = 24.0\n",
            "Episode 229: Reward = 105.0\n",
            "Episode 230: Reward = 108.0\n",
            "Episode 231: Reward = 79.0\n",
            "Episode 232: Reward = 121.0\n",
            "Episode 233: Reward = 116.0\n",
            "Episode 234: Reward = 16.0\n",
            "Episode 235: Reward = 16.0\n",
            "Episode 236: Reward = 16.0\n",
            "Episode 237: Reward = 104.0\n",
            "Episode 238: Reward = 14.0\n",
            "Episode 239: Reward = 147.0\n",
            "Episode 240: Reward = 123.0\n",
            "Episode 241: Reward = 30.0\n",
            "Episode 242: Reward = 19.0\n",
            "Episode 243: Reward = 95.0\n",
            "Episode 244: Reward = 97.0\n",
            "Episode 245: Reward = 62.0\n",
            "Episode 246: Reward = 45.0\n",
            "Episode 247: Reward = 91.0\n",
            "Episode 248: Reward = 22.0\n",
            "Episode 249: Reward = 124.0\n",
            "Episode 250: Reward = 22.0\n",
            "Episode 251: Reward = 12.0\n",
            "Episode 252: Reward = 133.0\n",
            "Episode 253: Reward = 19.0\n",
            "Episode 254: Reward = 16.0\n",
            "Episode 255: Reward = 111.0\n",
            "Episode 256: Reward = 20.0\n",
            "Episode 257: Reward = 153.0\n",
            "Episode 258: Reward = 27.0\n",
            "Episode 259: Reward = 61.0\n",
            "Episode 260: Reward = 133.0\n",
            "Episode 261: Reward = 18.0\n",
            "Episode 262: Reward = 131.0\n",
            "Episode 263: Reward = 75.0\n",
            "Episode 264: Reward = 11.0\n",
            "Episode 265: Reward = 17.0\n",
            "Episode 266: Reward = 21.0\n",
            "Episode 267: Reward = 108.0\n",
            "Episode 268: Reward = 60.0\n",
            "Episode 269: Reward = 117.0\n",
            "Episode 270: Reward = 97.0\n",
            "Episode 271: Reward = 19.0\n",
            "Episode 272: Reward = 29.0\n",
            "Episode 273: Reward = 63.0\n",
            "Episode 274: Reward = 100.0\n",
            "Episode 275: Reward = 17.0\n",
            "Episode 276: Reward = 57.0\n",
            "Episode 277: Reward = 64.0\n",
            "Episode 278: Reward = 83.0\n",
            "Episode 279: Reward = 56.0\n",
            "Episode 280: Reward = 107.0\n",
            "Episode 281: Reward = 13.0\n",
            "Episode 282: Reward = 17.0\n",
            "Episode 283: Reward = 44.0\n",
            "Episode 284: Reward = 156.0\n",
            "Episode 285: Reward = 103.0\n",
            "Episode 286: Reward = 101.0\n",
            "Episode 287: Reward = 67.0\n",
            "Episode 288: Reward = 114.0\n",
            "Episode 289: Reward = 113.0\n",
            "Episode 290: Reward = 113.0\n",
            "Episode 291: Reward = 65.0\n",
            "Episode 292: Reward = 108.0\n",
            "Episode 293: Reward = 19.0\n",
            "Episode 294: Reward = 96.0\n",
            "Episode 295: Reward = 147.0\n",
            "Episode 296: Reward = 109.0\n",
            "Episode 297: Reward = 119.0\n",
            "Episode 298: Reward = 131.0\n",
            "Episode 299: Reward = 19.0\n",
            "Episode 300: Reward = 128.0\n",
            "Episode 301: Reward = 157.0\n",
            "Episode 302: Reward = 119.0\n",
            "Episode 303: Reward = 173.0\n",
            "Episode 304: Reward = 25.0\n",
            "Episode 305: Reward = 182.0\n",
            "Episode 306: Reward = 147.0\n",
            "Episode 307: Reward = 140.0\n",
            "Episode 308: Reward = 161.0\n",
            "Episode 309: Reward = 128.0\n",
            "Episode 310: Reward = 181.0\n",
            "Episode 311: Reward = 160.0\n",
            "Episode 312: Reward = 12.0\n",
            "Episode 313: Reward = 188.0\n",
            "Episode 314: Reward = 89.0\n",
            "Episode 315: Reward = 35.0\n",
            "Episode 316: Reward = 227.0\n",
            "Episode 317: Reward = 51.0\n",
            "Episode 318: Reward = 170.0\n",
            "Episode 319: Reward = 157.0\n",
            "Episode 320: Reward = 170.0\n",
            "Episode 321: Reward = 164.0\n",
            "Episode 322: Reward = 170.0\n",
            "Episode 323: Reward = 155.0\n",
            "Episode 324: Reward = 163.0\n",
            "Episode 325: Reward = 18.0\n",
            "Episode 326: Reward = 151.0\n",
            "Episode 327: Reward = 164.0\n",
            "Episode 328: Reward = 193.0\n",
            "Episode 329: Reward = 147.0\n",
            "Episode 330: Reward = 177.0\n",
            "Episode 331: Reward = 183.0\n",
            "Episode 332: Reward = 151.0\n",
            "Episode 333: Reward = 21.0\n",
            "Episode 334: Reward = 178.0\n",
            "Episode 335: Reward = 191.0\n",
            "Episode 336: Reward = 188.0\n",
            "Episode 337: Reward = 168.0\n",
            "Episode 338: Reward = 53.0\n",
            "Episode 339: Reward = 174.0\n",
            "Episode 340: Reward = 163.0\n",
            "Episode 341: Reward = 13.0\n",
            "Episode 342: Reward = 113.0\n",
            "Episode 343: Reward = 180.0\n",
            "Episode 344: Reward = 149.0\n",
            "Episode 345: Reward = 193.0\n",
            "Episode 346: Reward = 208.0\n",
            "Episode 347: Reward = 196.0\n",
            "Episode 348: Reward = 24.0\n",
            "Episode 349: Reward = 194.0\n",
            "Episode 350: Reward = 173.0\n",
            "Episode 351: Reward = 259.0\n",
            "Episode 352: Reward = 343.0\n",
            "Episode 353: Reward = 264.0\n",
            "Episode 354: Reward = 226.0\n",
            "Episode 355: Reward = 231.0\n",
            "Episode 356: Reward = 14.0\n",
            "Episode 357: Reward = 16.0\n",
            "Episode 358: Reward = 235.0\n",
            "Episode 359: Reward = 270.0\n",
            "Episode 360: Reward = 22.0\n",
            "Episode 361: Reward = 65.0\n",
            "Episode 362: Reward = 276.0\n",
            "Episode 363: Reward = 346.0\n",
            "Episode 364: Reward = 170.0\n",
            "Episode 365: Reward = 403.0\n",
            "Episode 366: Reward = 204.0\n",
            "Episode 367: Reward = 25.0\n",
            "Episode 368: Reward = 311.0\n",
            "Episode 369: Reward = 449.0\n",
            "Episode 370: Reward = 117.0\n",
            "Episode 371: Reward = 395.0\n",
            "Episode 372: Reward = 367.0\n",
            "Episode 373: Reward = 276.0\n",
            "Episode 374: Reward = 965.0\n",
            "Episode 375: Reward = 1075.0\n",
            "Episode 376: Reward = 35.0\n",
            "Episode 377: Reward = 853.0\n",
            "Episode 378: Reward = 282.0\n",
            "Episode 379: Reward = 837.0\n",
            "Episode 380: Reward = 323.0\n",
            "Episode 381: Reward = 800.0\n",
            "Episode 382: Reward = 920.0\n",
            "Episode 383: Reward = 474.0\n",
            "Episode 384: Reward = 744.0\n",
            "Episode 385: Reward = 628.0\n",
            "Episode 386: Reward = 275.0\n",
            "Episode 387: Reward = 54.0\n",
            "Episode 388: Reward = 275.0\n",
            "Episode 389: Reward = 47.0\n",
            "Episode 390: Reward = 198.0\n",
            "Episode 391: Reward = 240.0\n",
            "Episode 392: Reward = 179.0\n",
            "Episode 393: Reward = 286.0\n",
            "Episode 394: Reward = 172.0\n",
            "Episode 395: Reward = 247.0\n",
            "Episode 396: Reward = 236.0\n",
            "Episode 397: Reward = 197.0\n",
            "Episode 398: Reward = 284.0\n",
            "Episode 399: Reward = 137.0\n",
            "Episode 400: Reward = 131.0\n",
            "Episode 401: Reward = 127.0\n",
            "Episode 402: Reward = 164.0\n",
            "Episode 403: Reward = 158.0\n",
            "Episode 404: Reward = 154.0\n",
            "Episode 405: Reward = 123.0\n",
            "Episode 406: Reward = 216.0\n",
            "Episode 407: Reward = 184.0\n",
            "Episode 408: Reward = 148.0\n",
            "Episode 409: Reward = 159.0\n",
            "Episode 410: Reward = 179.0\n",
            "Episode 411: Reward = 174.0\n",
            "Episode 412: Reward = 233.0\n",
            "Episode 413: Reward = 152.0\n",
            "Episode 414: Reward = 481.0\n",
            "Episode 415: Reward = 185.0\n",
            "Episode 416: Reward = 147.0\n",
            "Episode 417: Reward = 137.0\n",
            "Episode 418: Reward = 473.0\n",
            "Episode 419: Reward = 141.0\n",
            "Episode 420: Reward = 42.0\n",
            "Episode 421: Reward = 154.0\n",
            "Episode 422: Reward = 126.0\n",
            "Episode 423: Reward = 142.0\n",
            "Episode 424: Reward = 71.0\n",
            "Episode 425: Reward = 143.0\n",
            "Episode 426: Reward = 170.0\n",
            "Episode 427: Reward = 15.0\n",
            "Episode 428: Reward = 152.0\n",
            "Episode 429: Reward = 131.0\n",
            "Episode 430: Reward = 122.0\n",
            "Episode 431: Reward = 121.0\n",
            "Episode 432: Reward = 21.0\n",
            "Episode 433: Reward = 118.0\n",
            "Episode 434: Reward = 118.0\n",
            "Episode 435: Reward = 112.0\n",
            "Episode 436: Reward = 121.0\n",
            "Episode 437: Reward = 114.0\n",
            "Episode 438: Reward = 120.0\n",
            "Episode 439: Reward = 113.0\n",
            "Episode 440: Reward = 111.0\n",
            "Episode 441: Reward = 129.0\n",
            "Episode 442: Reward = 108.0\n",
            "Episode 443: Reward = 113.0\n",
            "Episode 444: Reward = 110.0\n",
            "Episode 445: Reward = 113.0\n",
            "Episode 446: Reward = 110.0\n",
            "Episode 447: Reward = 113.0\n",
            "Episode 448: Reward = 12.0\n",
            "Episode 449: Reward = 113.0\n",
            "Episode 450: Reward = 111.0\n",
            "Episode 451: Reward = 110.0\n",
            "Episode 452: Reward = 109.0\n",
            "Episode 453: Reward = 117.0\n",
            "Episode 454: Reward = 113.0\n",
            "Episode 455: Reward = 112.0\n",
            "Episode 456: Reward = 110.0\n",
            "Episode 457: Reward = 104.0\n",
            "Episode 458: Reward = 110.0\n",
            "Episode 459: Reward = 18.0\n",
            "Episode 460: Reward = 105.0\n",
            "Episode 461: Reward = 108.0\n",
            "Episode 462: Reward = 14.0\n",
            "Episode 463: Reward = 19.0\n",
            "Episode 464: Reward = 15.0\n",
            "Episode 465: Reward = 108.0\n",
            "Episode 466: Reward = 12.0\n",
            "Episode 467: Reward = 112.0\n",
            "Episode 468: Reward = 109.0\n",
            "Episode 469: Reward = 104.0\n",
            "Episode 470: Reward = 111.0\n",
            "Episode 471: Reward = 108.0\n",
            "Episode 472: Reward = 18.0\n",
            "Episode 473: Reward = 21.0\n",
            "Episode 474: Reward = 110.0\n",
            "Episode 475: Reward = 106.0\n",
            "Episode 476: Reward = 110.0\n",
            "Episode 477: Reward = 110.0\n",
            "Episode 478: Reward = 113.0\n",
            "Episode 479: Reward = 109.0\n",
            "Episode 480: Reward = 13.0\n",
            "Episode 481: Reward = 14.0\n",
            "Episode 482: Reward = 110.0\n",
            "Episode 483: Reward = 18.0\n",
            "Episode 484: Reward = 122.0\n",
            "Episode 485: Reward = 113.0\n",
            "Episode 486: Reward = 117.0\n",
            "Episode 487: Reward = 38.0\n",
            "Episode 488: Reward = 121.0\n",
            "Episode 489: Reward = 111.0\n",
            "Episode 490: Reward = 116.0\n",
            "Episode 491: Reward = 126.0\n",
            "Episode 492: Reward = 22.0\n",
            "Episode 493: Reward = 125.0\n",
            "Episode 494: Reward = 125.0\n",
            "Episode 495: Reward = 118.0\n",
            "Episode 496: Reward = 121.0\n",
            "Episode 497: Reward = 111.0\n",
            "Episode 498: Reward = 113.0\n",
            "Episode 499: Reward = 115.0\n",
            "Episode 500: Reward = 115.0\n",
            "Episode 1: Reward = 113.0\n",
            "Episode 2: Reward = 111.0\n",
            "Episode 3: Reward = 113.0\n",
            "Episode 4: Reward = 115.0\n",
            "Episode 5: Reward = 111.0\n",
            "Episode 6: Reward = 116.0\n",
            "Episode 7: Reward = 112.0\n",
            "Episode 8: Reward = 108.0\n",
            "Episode 9: Reward = 112.0\n",
            "Episode 10: Reward = 117.0\n",
            "Average reward over 10 episodes: 112.8\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run evaluation for cartpole here\n",
        "model = train_cartpole()\n",
        "evaluate_cartpole_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c4213b",
      "metadata": {
        "id": "b3c4213b"
      },
      "outputs": [],
      "source": [
        "class SnakeGame(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, size=10, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.cell_size = 30\n",
        "        self.screen_size = self.size * self.cell_size\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
        "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
        "\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "\n",
        "        self.snake = deque()\n",
        "        self.food = None\n",
        "        self.direction = [1, 0]\n",
        "\n",
        "        self.done = False\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.snake.clear()\n",
        "        mid = self.size // 2\n",
        "        self.snake.appendleft([mid, mid])\n",
        "        self.direction = [1, 0]\n",
        "        self._place_food()\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_init()\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Change reward schema to avoid the following\n",
        "        # 1) 180 degree turns\n",
        "        # 2) Wall collisions\n",
        "        # 3) Being slow at collecting food\n",
        "        if self.done:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        if (action == 0 and self.direction == 1) or \\\n",
        "           (action == 1 and self.direction == 0) or \\\n",
        "           (action == 2 and self.direction == 3) or \\\n",
        "           (action == 3 and self.direction == 2):\n",
        "            action = self.direction\n",
        "\n",
        "        self.direction = action\n",
        "\n",
        "        head_x, head_y = self.snake[0]\n",
        "        move = [(0, -1), (0, 1), (-1, 0), (1, 0)][self.direction]\n",
        "        new_head = (head_x + move[0], head_y + move[1])\n",
        "\n",
        "        if (not 0 <= new_head[0] < self.size) or (not 0 <= new_head[1] < self.size) or (new_head in self.snake):\n",
        "            self.done = True\n",
        "            return self._get_obs(), -10, True, {}\n",
        "\n",
        "        self.snake.insert(0, new_head)\n",
        "\n",
        "        if new_head == self.food:\n",
        "            self.score += 1\n",
        "            reward = 10\n",
        "            self._place_food()\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "            reward = -0.1  # small penalty to encourage faster food collection\n",
        "\n",
        "        return self._get_obs(), reward, False, {}\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Return an observation state, take inspiration from the observation_space attribute\n",
        "        pass\n",
        "        grid = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n",
        "        for x, y in self.snake:\n",
        "            grid[y, x, 0] = 1  # snake body in red channel\n",
        "        fx, fy = self.food\n",
        "        grid[fy, fx, 1] = 1  # food in green channel\n",
        "        return grid\n",
        "\n",
        "    def _place_food(self):\n",
        "        positions = set(tuple(p) for p in self.snake)\n",
        "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
        "        self.food = list(random.choice(empty)) if empty else None\n",
        "\n",
        "    def render(self):\n",
        "        if self.screen is None:\n",
        "            self._render_init()\n",
        "\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        for x, y in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (0, 255, 0),\n",
        "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (255, 0, 0),\n",
        "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def _render_init(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen:\n",
        "            pygame.quit()\n",
        "            self.screen = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3ffb7a",
      "metadata": {
        "id": "7d3ffb7a"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training logic for Snake Game here\n",
        "\n",
        "def train_snake(episodes=300):\n",
        "    env = SnakeGame(size=10)\n",
        "    state_dim = np.prod(env.observation_space.shape)\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    model = DQN(state_dim, action_dim)\n",
        "    target_model = DQN(state_dim, action_dim)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ExperienceBuffer(5000)\n",
        "\n",
        "    batch_size = 32\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    min_epsilon = 0.05\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        state = obs.flatten() / 1.0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, action_dim - 1)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_vals = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "                    action = torch.argmax(q_vals).item()\n",
        "\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            next_state = next_obs.flatten() / 1.0\n",
        "\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                next_q = target_model(next_states).max(1, keepdim=True)[0].detach()\n",
        "                target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "            print(f\"Episode {ep}: Total Reward = {total_reward}\")\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "    env.close()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e10565",
      "metadata": {
        "id": "a0e10565"
      },
      "outputs": [],
      "source": [
        "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
        "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7eb581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "1a7eb581",
        "outputId": "764d88e3-8d40-4dfb-efa3-3d566bc36f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward = -12.3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x300 and 100x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-1058711997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Run evaluation for Snake Game here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_snake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_snake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-31-228761662.py\u001b[0m in \u001b[0;36mtrain_snake\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnext_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-25-3976121842.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# TODO: Complete based on your implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x300 and 100x128)"
          ]
        }
      ],
      "source": [
        "# TODO: Run evaluation for Snake Game here\n",
        "model = train_snake()\n",
        "evaluate_snake_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4224fabe",
      "metadata": {
        "id": "4224fabe"
      },
      "outputs": [],
      "source": [
        "class ChaseEscapeEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dt = 0.1\n",
        "        self.max_speed = 0.4\n",
        "        self.agent_radius = 0.05\n",
        "        self.target_radius = 0.05\n",
        "        self.chaser_radius = 0.07\n",
        "        self.chaser_speed = 0.03\n",
        "\n",
        "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-1,\n",
        "            high=1,\n",
        "            shape=(8,),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.screen_size = 500\n",
        "        self.np_random = None\n",
        "\n",
        "        if render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
        "        while True:\n",
        "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
        "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
        "                return pos\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.agent_pos = self.sample_pos()\n",
        "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
        "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Decide how to pass the state (don't use pixel values)\n",
        "        pass\n",
        "\n",
        "    def _get_info(self):\n",
        "        return {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Add reward scheme\n",
        "        # 1) Try to make the agent stay within bounds\n",
        "        # 2) The agent shouldn't idle around\n",
        "        # 3) The agent should go for the reward\n",
        "        # 4) The agent should avoid the chaser\n",
        "\n",
        "        accel = (np.array(action) - 1) * 0.1\n",
        "        self.agent_vel += accel\n",
        "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
        "        self.agent_pos += self.agent_vel * self.dt\n",
        "        self.agent_pos = np.clip(self.agent_pos, -1, 1)\n",
        "\n",
        "        direction = self.agent_pos - self.chaser_pos\n",
        "        norm = np.linalg.norm(direction)\n",
        "        if norm > 1e-5:\n",
        "            self.chaser_pos += self.chaser_speed * direction / norm\n",
        "\n",
        "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
        "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
        "\n",
        "        reward = 0.0\n",
        "        terminated = False\n",
        "\n",
        "        if dist_to_target < self.agent_radius + self.target_radius:\n",
        "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "\n",
        "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
        "            terminated = True\n",
        "\n",
        "        return self._get_obs(), reward, terminated, False, self._get_info()\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode != \"human\":\n",
        "            return\n",
        "\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                self.close()\n",
        "\n",
        "        self.screen.fill((255, 255, 255))\n",
        "\n",
        "        def to_screen(p):\n",
        "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
        "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
        "            return x, y\n",
        "\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def close(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eca1390",
      "metadata": {
        "id": "9eca1390"
      },
      "outputs": [],
      "source": [
        "# TODO: Train and evaluate CatMouseEnv"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}